{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Acquisition\n",
    "## Article Page Views API\n",
    "This notebook illustrates how to access page view data using the [Wikimedia REST API](https://www.mediawiki.org/wiki/Wikimedia_REST_API). \n",
    "\n",
    "## License\n",
    "This code was adapted from an example developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.3 - August 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import requests\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the list of article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading in the article titles ###\n",
    "input_data = pd.read_csv(\"rare-disease_cleaned.AUG.2024.csv\")\n",
    "\n",
    "# Extract the disease names\n",
    "disease_names = input_data['disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<anetzley@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is the list of diseases that we previously read in\n",
    "ARTICLE_TITLES = disease_names \n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",   # Starting on July 1, 2015\n",
    "    \"end\":         \"2024093000\"    # Ending on September 30, 2024\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the API Call\n",
    "The following three functions enable the API calling. The first, \"request_pageviews_per_article\", executes the API call for that specific article. The second, \"store_pageviews_per_access\", generates a new dictionary storing all of the pageviews per article for a specific access type, call \"request_pageviews_per_article\" in a loop. Lastly, \"combine_pageviews\" loops through the three raw, gathered access pageviews, and combines them into the three final pageview stratifications requested: and desktop, all mobile, and combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def store_pageviews_per_access(access, article_titles = ARTICLE_TITLES, template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE):\n",
    "    #Create a new dictionary to hold all of the pageviews for this access type\n",
    "    all_views = {}\n",
    "    for art in article_titles:\n",
    "        template['access'] = access\n",
    "        #print(\"Getting pageview data for: \", art)\n",
    "        \n",
    "        #Execute the API call\n",
    "        views = request_pageviews_per_article(art, request_template = template)\n",
    "\n",
    "        #Store the views into the dictionary\n",
    "        if 'items' in views:\n",
    "            all_views[art] = views['items']\n",
    "        else:\n",
    "            all_views[art] = None\n",
    "\n",
    "    return all_views\n",
    "\n",
    "def combine_pageviews(desktop_all_views, mobile_app_all_views, mobile_web_all_views, article_titles = ARTICLE_TITLES):\n",
    "    #Initialize empty dicts to hold the combined views data\n",
    "    mobile_all_views = {}\n",
    "    combined_all_views = {}\n",
    "\n",
    "    #Loop through each article and combine views to generate the mobile all_views and combined_all_views\n",
    "    for article_name in article_titles:\n",
    "\n",
    "        #Fill in the dictionaries with the rest of the data from the mobile app source to serve as background\n",
    "        mobile_all_views[article_name] = copy.deepcopy(mobile_app_all_views[article_name])\n",
    "        combined_all_views[article_name] = copy.deepcopy(mobile_app_all_views[article_name])\n",
    "        #print(mobile_app_all_views[name])\n",
    "        #print(mobile_all_views[name])\n",
    "\n",
    "        #Iterate through each month of data for the article and combine views\n",
    "        if mobile_web_all_views[article_name] is not None:\n",
    "            for i, key in enumerate(mobile_web_all_views[article_name]):\n",
    "\n",
    "                #Extract the corresponding viewcounts from the other two sources\n",
    "                app_views = mobile_app_all_views[article_name][i]['views']\n",
    "                desktop_views = desktop_all_views[article_name][i]['views']\n",
    "                web_views = key['views']\n",
    "                print('App Views:', app_views)\n",
    "                print('Desktop Views:', desktop_views)\n",
    "                print('Web Views:', web_views)\n",
    "\n",
    "                #print(mobile_all_views[article_name][i])\n",
    "\n",
    "                #Assign the aggregate view counts to each respective dict\n",
    "                mobile_all_views[article_name][i]['views'] = app_views + web_views\n",
    "                combined_all_views[article_name][i]['views'] = app_views + web_views + desktop_views\n",
    "\n",
    "                print('Mobile All Views:',mobile_all_views[article_name][i]['views'])\n",
    "                print('Combined All Views:',combined_all_views[article_name][i]['views'])\n",
    "                print('\\n')\n",
    "\n",
    "                #Lastly, remove the 'access' key from all of the dicts\n",
    "                desktop_all_views[article_name][i].pop('access', None)\n",
    "                mobile_all_views[article_name][i].pop('access', None)\n",
    "                combined_all_views[article_name][i].pop('access', None)\n",
    "            \n",
    "    return desktop_all_views, mobile_all_views, combined_all_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "The main section below performs the actual analysis, executing the API calls and combining the pageviews using the functions described above. It stores the final processed data to three separate json files located in the \"data_clean\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the raw API Calls\n",
    "desktop_all_views = store_pageviews_per_access('desktop')\n",
    "mobile_app_all_views = store_pageviews_per_access('mobile-app')\n",
    "mobile_web_all_views = store_pageviews_per_access('mobile-web')\n",
    "\n",
    "# with open('mapp_temp.json', 'w') as file:\n",
    "#     json.dump(mobile_app_all_views, file, indent=4)\n",
    "\n",
    "# with open('desktop_temp.json', 'w') as file:\n",
    "#     json.dump(desktop_all_views, file, indent=4)\n",
    "\n",
    "# with open('mweb_temp.json', 'w') as file:\n",
    "#     json.dump(mobile_web_all_views, file, indent=4)\n",
    "\n",
    "#Combine pageviews\n",
    "desktop_all_views, mobile_all_views, combined_all_views = combine_pageviews(desktop_all_views, mobile_app_all_views, mobile_web_all_views)\n",
    "\n",
    "# #Printing a few results for a sanity check\n",
    "# print(f\"Collected {len(mobile_all_views[ARTICLE_TITLES[0]])} months of pageview data\")\n",
    "# for name in ARTICLE_TITLES[:4]:\n",
    "#     for month in mobile_all_views[name]:\n",
    "#         print(json.dumps(month,indent=4))\n",
    "\n",
    "#Write the outputs to the three JSON files\n",
    "with open('rare-disease_monthly_mobile_201507_202409.json', 'w') as file:\n",
    "    json.dump(mobile_all_views, file, indent=4)\n",
    "\n",
    "with open('rare-disease_monthly_desktop_201507_202409.json', 'w') as file:\n",
    "    json.dump(desktop_all_views, file, indent=4)\n",
    "\n",
    "with open('rare-disease_monthly_combined_201507_202409.json', 'w') as file:\n",
    "    json.dump(combined_all_views, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DATA557",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
